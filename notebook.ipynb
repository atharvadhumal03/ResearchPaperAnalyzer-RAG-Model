{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "846d9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30005882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetching the API key\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Verifing the key loaded\n",
    "print(\"API Key loaded successfully!\" if gemini_api_key else \"API Key not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60723633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini LLM initialized!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initializing Gemini\n",
    "geminillm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    google_api_key=gemini_api_key,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Gemini LLM initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "223f877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28 pages\n",
      "\n",
      "First page content preview:\n",
      "Retrieval-Augmented Generation for\n",
      "Knowledge-Intensive NLP Tasks\n",
      "Patrick Lewis†‡, Ethan Perez⋆,\n",
      "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
      "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
      "†Facebook AI Research;‡University College London;⋆New York University;\n",
      "plewis@fb.com\n",
      "Abstract\n",
      "Large pre-trained language models have been shown to store factual knowledge\n",
      "in their parameters, and achieve state-of-the-art results when ﬁ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"testPaper.pdf\" \n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(f\"Loaded {len(pages)} pages\\n\")\n",
    "print(f\"First page content preview:\\n{pages[0].page_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c87ae9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chunks - 96\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Creating a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # chars per chunk\n",
    "    chunk_overlap=200,      # overlap between chunks\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "print(f\"total chunks - {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41664e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings created\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initializing embeddings model\n",
    "HuggingFaceembeddingsModel = HuggingFaceEmbeddings(\n",
    "       model_name=\"all-MiniLM-L6-v2\"\n",
    "   )\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=HuggingFaceembeddingsModel,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"embeddings created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1173a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings created: 96\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of embeddings created: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a439f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation for\n",
      "Knowledge-Intensive NLP Tasks\n",
      "Patrick Lewis†‡, Ethan Perez⋆,\n",
      "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
      "Mike Lewis†, W\n",
      "\n",
      "Embedding dimension: 384\n",
      "First 5 values: [-0.06688090413808823, -0.03467119485139847, -0.026010597124695778, 0.08180363476276398, 0.01612071320414543]\n"
     ]
    }
   ],
   "source": [
    "# Testing embeddings\n",
    "print(chunks[0].page_content[:200])\n",
    "sample_embedding = HuggingFaceembeddingsModel.embed_query(chunks[0].page_content)\n",
    "\n",
    "print(f\"\\nEmbedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"First 5 values: {sample_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77bc3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is this paper about?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Chunk 1:\n",
      "speciﬁc by a large margin. Table 3 shows typical generations from each model.\n",
      "Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform\n",
      "best because it can generate responses that combine content from several documents. Figure 2 shows\n",
      "an example. When generating \n",
      "...\n",
      "\n",
      "Chunk 2:\n",
      "https://www.aclweb.org/anthology/P17-1171.\n",
      "[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\n",
      "Jonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the\n",
      "55th Annual Meeting of the Association for Computational Linguistics (Volu\n",
      "...\n",
      "\n",
      "Chunk 3:\n",
      "ard of wikipedia: Knowledge-powered conversational agents. In International Conference on\n",
      "Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\n",
      "[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\n",
      "Cho. SearchQA: A New Q&A Dataset Augme\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search\n",
    "query = \"What is this paper about?\"\n",
    "\n",
    "# Retrieve top 3 most relevant chunks\n",
    "relevant_chunks = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk.page_content[:300])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17475ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to join documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfb31492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | geminillm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35c21bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The research paper \"Coarse-to-fine question answering for long documents\" by Choi et al. was published in July 2017. It appeared in the Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"When was the research paper published?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
